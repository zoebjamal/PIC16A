{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2: Markov Models of Natural Language\n",
    "\n",
    "*Fill in your name and the names of any students who helped you below.* \n",
    "\n",
    "> I affirm that I personally wrote the text, code, and comments in this homework assignment. \n",
    "\n",
    "> I did not recieve help on this assignment.\n",
    "\n",
    "- *By Zoeb Jamal*\n",
    "- Jan. 20th, 2021\n",
    "\n",
    "\n",
    "## Language Models\n",
    "\n",
    "Many of you may have encountered the output of machine learning models which, when \"seeded\" with a small amount of text, produce a larger corpus of text which is expected to be similar or relevant to the seed text. For example, there's been a lot of buzz about the new [GPT-3 model](https://en.wikipedia.org/wiki/GPT-3), related to its [carbon footprint](https://www.forbes.com/sites/robtoews/2020/06/17/deep-learnings-climate-change-problem/#2781c1b16b43), [bigoted tendencies](https://medium.com/fair-bytes/how-biased-is-gpt-3-5b2b91f1177), and, yes, impressive (and often [humorous](https://aiweirdness.com/)) [ability to replicate human-like text in response to prompts.](https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/) \n",
    "\n",
    "We are not going to program a complicated deep learning model, but we will construct a much simpler language model that performs a similar task. Using tools like iteration and dictionaries, we will create a family of **Markov language models** for generating text. For the purposes of this assignment, an $n$-th order Markov model is a function that constructs a string of text one letter at a time, using only knowledge of the most recent $n$ letters. You can think of it as a writer with a \"memory\" of $n$ letters. \n",
    "\n",
    "## Data\n",
    "\n",
    "Our training text for this exercise comes from the first 10 chapters of Jane Austen's novel *Emma*, which I retrieved from the archives at [Project Gutenberg](https://www.gutenberg.org/files/158/158-h/158-h.htm#link2H_4_0001). Intuitively, we are going to write a program that \"writes like Jane Austen,\" albeit in a very limited sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Today when I walked into my economics class I saw something I dread every time I close my eyes. Someone had brought their new gaming laptop to class. The Forklift he used to bring it was still running idle at the back. I started sweating as I sat down and gazed over at the 700lb beast that was his laptop. He had already reinforced his desk with steel support beams and was in the process of finding an outlet for a power cable thicker than Amy Schumer's thigh. I start shaking. I keep telling myself I'm going to be alright and that there's nothing to worry about. He somehow finds a fucking outlet. Tears are running down my cheeks as I send my last texts to my family saying I love them. The teacher starts the lecture, and the student turns his laptop on. The colored lights on his RGB Backlit keyboard flare to life like a nuclear flash, and a deep humming fills my ears and shakes my very soul. The entire city power grid goes dark. The classroom begins to shake as the massive fans begin to spin. In mere seconds my world has gone from vibrant life, to a dark, earth shattering void where my body is getting torn apart by the 150mph gale force winds and the 500 decibel groan of the cooling fans. As my body finally surrenders, I weep, as my school and my city go under. I fucking hate gaming laptops\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments and Docstrings\n",
    "\n",
    "This is the first homework assignment in which you will be graded in part on the quality of your documentation and explanation of your code. Here's what we expect: \n",
    "\n",
    "- **Comments**: Use comments liberally to explain the purpose of short snippets of code. \n",
    "- **Docstrings**: Functions (and, later, classes) should always be accompanied by a *docstring*. Briefly, the docstring should provide enough information that a user could correctly use your function ***without seeing the code.*** In somewhat more detail, the docstring should include the following information: \n",
    "    - One or more sentences describing the overall purpose of the function. \n",
    "    - An explanation of each of the inputs, including what they mean, their required data types, and any additional assumptions made about them.\n",
    "    - An explanation of the output. \n",
    "    \n",
    "In future homeworks, as well as on exams, we will be looking for clear and informative comments and docstrings. \n",
    "\n",
    "## Code Structure\n",
    "\n",
    "In general, there are many good ways to solve a given problem. However, just getting the right result isn't enough to guarantee that your code is of high quality. Check the logic of your solutions to make sure that: \n",
    "\n",
    "- You aren't making any unnecessary steps, like creating variables you don't use. \n",
    "- You are effectively making use of the tools in the course, especially control flow. \n",
    "- Your code is readable. Each line is short (under 80 characters), and doesn't have long tangles of functions or `()` parentheses. \n",
    "\n",
    "Ok, let's go! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Write a function called `count_characters()` that counts the number of times each character appears in a user-supplied string `s`. Your function should loop over each element of the string, and sequentually update a `dict` whose keys are characters and whose values are the number of occurrences seen so far. You may know of other ways to achieve the same result. However, you should use the loop approach, since this will generalize to the next exercise. \n",
    "\n",
    "*Note: while the construct `for letter in s:` will work for this exercise, it will not generalize to the next one. Use `for i in range(len(s)):` instead.* \n",
    "\n",
    "### Example usage: \n",
    "\n",
    "```python\n",
    "count_characters(\"tortoise\")\n",
    "{'t' : 2, 'o' : 2, 'r' : 1, 'i' : 1, 's' : 1, 'e' : 1}\n",
    "```\n",
    "\n",
    "***Hint***: Yes, you did a problem very similar to this one on HW1. \n",
    "\n",
    "\n",
    "### Your Solution\n",
    "To solve this problem, I will follow the same approach as in hw1, problem 2\n",
    "- I will write a function to convert a string to a dictionary. I will create an empty `dict` in the body of the function and use a for loop to go through the elements of the string and write them in to the dictionary. In order to create the key-value pair, I will set each key equal to the number of times that element occurs in the original string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t': 2, 'o': 2, 'r': 1, 'i': 1, 's': 1, 'e': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write count_characters() here \n",
    "def count_characters(myString):\n",
    "    \"\"\"\n",
    "    counts the number of times each character appears in a user-supplied string\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    myString = string that user wants to convert to dict\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    d = dict whose keys are each unique character and corresponding value is the number of times it appears\n",
    "    \"\"\"\n",
    "    d = {} # creating emtpy dict\n",
    "    for i in range(len(myString)): # looping through the string\n",
    "        d[myString[i]] = myString.count(myString[i])\n",
    "        # each key of d is set to the i'th element of myString and its value is set to the number of times it occurs\n",
    "        # I know that this will result in multiple occurences of each element of myString, but since dict keys are\n",
    "        # dropped if they are repeated, this is not an issue\n",
    "    return d\n",
    "\n",
    "count_characters(\"tortoise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "An `n`-*gram* is a sequence of `n` letters. For example, `bol` and `old` are the two 3-grams that occur in the string `bold`. \n",
    "\n",
    "Write a function called `count_ngrams()` that counts the number of times each `n`-gram occurs in a string, with `n` specified by the user and with default value `n = 1`. You should be able to do this by making only a small modification to `count_characters()`. \n",
    "\n",
    "### Example usage: \n",
    "\n",
    "```python\n",
    "count_ngrams(\"tortoise\", n = 2)\n",
    "```\n",
    "```\n",
    "{'to': 2, 'or': 1, 'rt': 1, 'oi': 1, 'is': 1, 'se': 1} # output\n",
    "```\n",
    "\n",
    "### Your Solution\n",
    "To solve this problem, I will follow the same approach as in problem 1, with some minor adjustments\n",
    "- I will need the keys of `d` to be `n`-grams instead of the `i`th element of the string. To do this, I can make a substring and set that as the `i`th key.\n",
    "- Counting the number of times that `n`-gram appears will remain the same, but with the argument for the `.count()` method modified to reflect the substring\n",
    "- I ran into a problem where I was getting keys in my dict that were not of the proper length. I think this had to do with how I was setting the keys of `d`. Essentially, the last `n - 1` keys of the dict would be of a shorter length than needed. For example, \n",
    "```python\n",
    "count_ngrams(\"tortoise\", n = 3)\n",
    "```\n",
    "```\n",
    "{'tor': 1, 'ort': 1, 'rto': 1, 'toi': 1, 'ois': 1, 'ise': 1, 'se': 1, 'e': 1}\n",
    "```\n",
    "- To solve this problem, I added an if statement that checks if the substring we are about to create a key for is of the specified `n`-gram length. If it isn't, it will skip this value until `i` reaches the end of the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'to': 2, 'or': 1, 'rt': 1, 'oi': 1, 'is': 1, 'se': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write count_ngrams() here\n",
    "def count_ngrams(myString, n = 1):\n",
    "    \"\"\"\n",
    "    counts the number of n-grams in a string\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    myString = string that the user enters\n",
    "    n = the number of characters the user wants in each n-gram -> set to 1 by default\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    d = dict whose keys are the n-grams and values are the number of times they appear\n",
    "    \"\"\"\n",
    "    d = {} # initialize empty dict\n",
    "    for i in range(len(myString)): # loop through the string\n",
    "        if len(myString[i : i + n]) == n: # need to make sure we are getting the correct length n-grams\n",
    "            d[myString[i : i + n]] = myString.count(myString[i : i + n])\n",
    "            # each key of d is set to a substring that contains n characters in myString -> value is number of occurences\n",
    "            # I know that this will result in multiple occurences of each element of myString, but since dict keys are\n",
    "            # dropped if they are repeated, this is not an issue\n",
    "    return d\n",
    "\n",
    "count_ngrams(\"tortoise\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Now we are going to use our `n`-grams to generate some fake text according to a Markov model. Here's how the Markov model of order `n` works: \n",
    "\n",
    "### A. Compute (`n`+1)-gram occurrence frequencies\n",
    "\n",
    "You have already done this in Exercise 2!  \n",
    "\n",
    "### B. Pick a starting (`n`+1)-gram\n",
    "\n",
    "The starting (`n`+1)-gram can be selected at random, or the user can specify it. \n",
    "\n",
    "### C. Generate Text\n",
    "\n",
    "Now we generate text one character at a time. To do so:\n",
    "\n",
    "1. Look at the most recent `n` characters in our generated text. Say that `n = 3` and the 3 most recent character are `the`. \n",
    "2. We then look at our list of `n+1`-grams, and focus on grams whose first `n` characters match. Examples matching `the` include `them`, `the `, `thei`, and so on. \n",
    "3. We pick a random one of these `n+1`-grams, weighted according to its number of occurrences. \n",
    "4. The final character of this new `n+1` gram is our next letter. \n",
    "\n",
    "For example, if there are 3 occurrences of `them`, 4 occurrences of `the `, and 1 occurrences of `thei` in the n-gram dictionary, then our next character is `m` with probabiliy 3/8, `[space]` with probability 1/2, and `i` with probability `1/8`. \n",
    "\n",
    "**Remember**: the ***3rd***-order model requires you to compute ***4***-grams. \n",
    "\n",
    "## What you should do\n",
    "\n",
    "Write a function that generates synthetic text according to an `n`-th order Markov model. It should have the following arguments: \n",
    "\n",
    "- `s`, the input string of real text. \n",
    "- `n`, the order of the model. \n",
    "- `length`, the size of the text to generate. Use a default value of 100. \n",
    "-  `seed`, the initial string that gets the Markov model started. I used `\"Emma Woodhouse\"` (the full name of the protagonist of the novel) as my `seed`, but any subset of `s` of length `n+1` or larger will work. \n",
    "\n",
    "Demonstrate the output of your function for a couple different choices of the order `n`. \n",
    "\n",
    "\n",
    "## Expected Output\n",
    "\n",
    "Here are a few examples of the output of this function. Because of randomness, your results won't look exactly like this, but they should be qualitatively similar. \n",
    "\n",
    "```python\n",
    "markov_text(s, n = 2, length = 200, seed = \"Emma Woodhouse\")\n",
    "```\n",
    "```\n",
    "Emma Woodhouse ne goo thimser. John mile sawas amintrought will on I kink you kno but every sh inat he fing as sat buty aft from the it. She cousency ined, yount; ate nambery quirld diall yethery, yould hat earatte\n",
    "```\n",
    "```python\n",
    "markov_text(s, n = 4, length = 200, seed = \"Emma Woodhouse\")\n",
    "```\n",
    "\n",
    "```\n",
    "Emma Woodhouse!”—Emma, as love,            Kitty, only this person no infering ever, while, and tried very were no do be very friendly and into aid,    Man's me to loudness of Harriet's. Harriet belonger opinion an\n",
    "```\n",
    "\n",
    "```python\n",
    "markov_text(s, n = 10, length = 200, seed = \"Emma Woodhouse\")\n",
    "```\n",
    "\n",
    "```\n",
    "Emma Woodhouse's party could be acceptable to them, that if she ever were disposed to think of nothing but good. It will be an excellent charade remains, fit for any acquainted with the child was given up to them.\n",
    "```\n",
    "\n",
    "## Notes and Hints\n",
    "\n",
    "***Hint***: A good function for performing the random choice is the `choices()` function in the `random` module. You can use it like this: \n",
    "\n",
    "```python\n",
    "import random\n",
    "\n",
    "options = [\"One\", \"Two\", \"Three\"]\n",
    "weights = [1, 2, 3] # \"Two\" is twice as likely as \"One\", \"Three\" three times as likely. \n",
    "\n",
    "random.choices(options, weights) \n",
    "```\n",
    "\n",
    "```\n",
    "['One'] # output\n",
    "```\n",
    "\n",
    "The first and second arguments must be lists of equal length. Note also that the return value is a list -- if you want the value *in* the list, you need to get it out via indexing.  \n",
    "\n",
    "***Hint***: The first thing your function should do is call `count_ngrams` above to generate the required dictionary. Then, handle the logic described above in the main loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write markov_text() here\n",
    "import random\n",
    "def markov_text(s, n, length = 100, seed = \"Emma Woodhouse\"):\n",
    "    \"\"\"\n",
    "    Generates fake text according to a Markov model. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    s = user supplied string\n",
    "    n = order of the model\n",
    "    length = the length of text to create (default = 100 characters)\n",
    "    seed = the string to get the model to start\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    generated_text = the markov text created by this function\n",
    "    \"\"\"\n",
    "    markov_dict = count_ngrams(s, n + 1) # creating a dict of ngrams for the input string\n",
    "    markov_keys = list(markov_dict.keys()) # making a list of the keys and values in the dict\n",
    "    markov_values = list(markov_dict.values())\n",
    "    generated_text = seed # initializing the generated_text string to the seed\n",
    "    for i in range(length): # need to loop for however many times the user requires\n",
    "        recent = generated_text[-n:] # getting the  most recent n characters\n",
    "        options = [] # making empty lists that will hold the ngrams and frequencies that match with recent\n",
    "        weights = []\n",
    "        for i in range(len(markov_keys)): # looping through all the ngrams\n",
    "            if recent == markov_keys[i][0 : n]: # checking if recent matches with the first n characters of the ngrams\n",
    "                options.append(markov_keys[i]) # adding the matches to the options list\n",
    "                weights.append(markov_values[i]) # adding the frequences to the weights list\n",
    "        \n",
    "        random_choice = random.choices(options, weights) # getting a random ngram\n",
    "        random_gram = random_choice[0] # converting random ngram to string\n",
    "        generated_text += random_gram[-1] # adding the last character of random_gram to our generated_text\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Today when I walked into my economics class I saw something I dread every time I close my eyes. Someone had brought their new gaming laptop to class. The Forklift he used to bring it was still running idle at the back. I started sweating as I sat down and gazed over at the 700lb beast that was his laptop. He had already reinforced his desk with steel support beams and was in the process of finding an outlet for a power cable thicker than Amy Schumer's thigh. I start shaking. I keep telling myself I'm going to be alright and that there's nothing to worry about. He somehow finds a fucking outlet. Tears are running down my cheeks as I send my last texts to my family saying I love them. The teacher starts the lecture, and the student turns his laptop on. The colored lights on his RGB Backlit keyboard flare to life like a nuclear flash, and a deep humming fills my ears and shakes my very soul. The entire city power grid goes dark. The classroom begins to shake as the massive fans begin to spin. In mere seconds my world has gone from vibrant life, to a dark, earth shattering void where my body is getting torn apart by the 150mph gale force winds and the 500 decibel groan of the cooling fans. As my body finally surrenders, I weep, as my school and my city go under. I fucking hate gaming laptop to class. The Forklift he\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try out your function for a few different values of n\n",
    "markov_text(s, n = 10, length = 1308, seed = \"Today when I walked into \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Approximately :o: 15 :b: people :man::woman: have died :skull: in HALF :skull: from COVID :mask::heart_eyes_cat::imp::japanese_ogre:. Not convinced :thinking:?Trump :tangerine::person_in_tuxedo_tone2::women_with_f'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try another value of n! \n",
    "markov_text(s, n = 5, length = 200, seed = \"Approximately\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Approximately :o: 72813% more jobs :briefcase: in HALF :skull: of his :sweat_drops::eyes: FIRST :first_place: TERM :page_facing_up: than Biden :bangbang: has during in his :sweat_drops: glorious :innocent: reign :'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try third value! \n",
    "markov_text(s, n = 10, length = 200, seed = \"Approximately\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Using a `for`-loop, print the output of your function for `n` ranging from `1` to `10`. \n",
    "\n",
    "Then, write down a few observations. How does the generated text depend on `n`? How does the time required to generate the text depend on `n`? Do your best to explain each observation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 1\n",
      "47.2 s ± 62.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Emma Woodhouse benght paf w tha, y, ncineriry ut ovet d Kn oeleld, grssh choorpun igr anthot is tt he atot whe bely nge mitid weeryoor o onditon r n; e itillirand bofuchaby haingr I owa visig buctllo h cersu ve:—ss\n",
      "\n",
      "n = 2\n",
      "40.1 s ± 5.55 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Emma Woodhousende.”Thing an way ing elphigir ther knintend was bery gones lon of Emme it frommay gend dead not he fave hat gings Isand thal hasty yed a re reany dearcesis itin al gire sookink wast itiond hertuntim \n",
      "\n",
      "n = 3\n",
      "34.2 s ± 3.16 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Emma Woodhouse she him—norand which me of Miss only expectly othink I would happer on atter, will Fairy)—conce of propersual with an in olded as she world, claim this very sincliness mannot day, point.”“I subjectio\n",
      "\n",
      "n = 4\n",
      "31 s ± 2.15 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Emma Woodhouse's farth! the only obliged tempered receiving downright be and in agains, and I am so much a prospectable, know—but it is very narrow, you with her doing ill-equirest, and longer to loudness of it. He\n",
      "\n",
      "n = 5\n",
      "28.2 s ± 417 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Emma Woodhouse had done day with taking, I supportunate in Brunswick Square. However, Miss Woodhouse away very credit for her. As a woman's daughters for a few moments to marriet transcribe. If I did not mind of he\n",
      "\n",
      "n = 6\n",
      "30 s ± 5.74 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Emma Woodhouse gratitude of thin quarter of walking it. She lover.—“The best know what she to be Harriet, mine it had been entirely with half a most perfect unreserve. Mr. Knightley, feelings at Randalls. He is an \n",
      "\n",
      "n = 7\n",
      "29.1 s ± 2.66 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Emma Woodhouse, almost immediately that I made the match I felt the book again, and when there are no husband's house and of comfortable, and domestic comfortable age, and Miss Nash would—for Miss Taylor.”“Thank yo\n",
      "\n",
      "n = 8\n",
      "30.1 s ± 4.98 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Emma Woodhouse, who had not fancy her ungrateful spirit, she had always did him good; and he has left my picture, which in specimens of your making a good wife;—and you would wish to go on, and he had played with i\n",
      "\n",
      "n = 9\n",
      "25.5 s ± 435 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Emma Woodhouse what she liked—but she could ride to London, chuse the frame, and given many little Welch cow, a very good-humoured, cheerful—Mr. Weston is such a definition? Harriet a beautiful creature, and a very\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# code here\n",
    "for i in range(1, 10):\n",
    "    print(\"n = \" + str(i))\n",
    "    %timeit markov_text(s, n = i, length = 200, seed = \"Emma Woodhouse\")\n",
    "    print(markov_text(s, n = i, length = 200, seed = \"Emma Woodhouse\") + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 10\n",
      "26.1 s ± 293 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Emma Woodhouse at last was off; but Mr. Knightley. “Success supposes endeavouring for the effect of shade, you know—in a joke—it is all a joke. We always speaks to the public favour; and she had no intellects of Hi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"n = 10\")\n",
    "%timeit markov_text(s, n = 10, length = 200, seed = \"Emma Woodhouse\")\n",
    "print(markov_text(s, n = 10, length = 200, seed = \"Emma Woodhouse\") + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- We can see that the \"correctness\"/complexity of the text increases significantly as n increases. While it is pretty much gibberish for lower n values, when you run the function with n =10, you actually get cohesive sentence structure (syntax and meaning may be lacking but at least spelling is correct). I think this is because the `n+1`-grams are higher (ie for n = 10 we would be using 11grams). In this case, the keys of the dict created by `count_ngrams()` will be much more reflective of what actual english looks like and therefore produce more \"correct\" words when we add new characters. \n",
    "- We can see that the time required goes down significantly as n increases from 1 to 10. The time required was 47.2 s ± 62.6 ms for n = 1, compared to 26.1 s ± 293 ms for n = 10. In cases where n is low, the `n+1`grams will also be really short. this leads to having more keys compared to the dict for `n+`grams where `n` is large. Since the dict for small `n` is much larger, it takes longer for the function to loop through the `markov_keys` list to see if our most recent `n` characters are a match. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
